{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fa202fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spektral\n",
    "from spektral.data import BatchLoader\n",
    "from spektral.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf8b2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from spektral.data import Dataset, DisjointLoader, Graph\n",
    "from spektral.layers import GCSConv, GlobalAvgPool,GeneralConv,GCNConv,GraphSageConv, GATConv, GlobalMaxPool,GlobalSumPool\n",
    "from spektral.transforms.normalize_adj import NormalizeAdj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64099e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.models.gcn import GCN\n",
    "from spektral.transforms import LayerPreprocess\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a913691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from spektral.data.loaders import SingleLoader\n",
    "from spektral.datasets.citation import Citation\n",
    "from spektral.layers import GCNConv, GraphMasking,ECCConv, GlobalSumPool, LaPool\n",
    "from spektral.models.gcn import GCN\n",
    "from spektral.transforms import LayerPreprocess, Degree, OneHotLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5335d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d785bc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 37)\n",
      "<class 'int'>\n",
      "37\n",
      "<class 'int'>\n",
      "37\n",
      "[6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_0/8qpjvkp91253d9fjqcjqgk200000gn/T/ipykernel_1681/2017222172.py:17: DeprecationWarning: \n",
      "\n",
      "The scipy.sparse array containers will be used instead of matrices\n",
      "in Networkx 3.0. Use `from_scipy_sparse_array` instead.\n",
      "  h = nx.from_scipy_sparse_matrix(graphs[0])\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "import networkx as nx\n",
    "from networkx import degree\n",
    "\n",
    " \n",
    "\n",
    "matFile = sio.loadmat('ENZYMES.mat', squeeze_me=True)\n",
    "\n",
    "\n",
    "labels = matFile['label']\n",
    "\n",
    "graphs = matFile['graph_struct']['am']\n",
    "\n",
    "print(graphs[0].shape)\n",
    "\n",
    "h = nx.from_scipy_sparse_matrix(graphs[0])\n",
    "print(type(len(h)))\n",
    "print(len(h))\n",
    "print(type(len(h.nodes)))\n",
    "print(len(h.nodes))\n",
    "print(labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# for graph_struct in graphs:\n",
    "\n",
    "#     am = graph_struct['am']\n",
    "\n",
    "#     G = nx.from_scipy_sparse_array(am)\n",
    "\n",
    "#     # nl = graph_struct['nl'] # node tag\n",
    "\n",
    "#     # al = graph_struct['al'] # adjacency list\n",
    "\n",
    "#     print(len(G.nodes))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f6f06c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset of five random graphs.\n",
    "    \"\"\"\n",
    "    def __init__(self, graphs, labels, **kwargs):\n",
    "        self.graphs = graphs\n",
    "        self.labels = labels\n",
    "        #self.graph = graph\n",
    "        #self.label = label\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    \n",
    "    def read(self):\n",
    "        output = []\n",
    "        ds = zip(self.graphs,self.labels)\n",
    "        for graph,label in ds:\n",
    "#             G = nx.from_scipy_sparse_matrix(graph)\n",
    "#             deg_list = []\n",
    "#             for i in range(len(G)):\n",
    "#                 deg_list.append(G.degree[i])\n",
    "#             q = np.array(deg_list)\n",
    "#             r = np.zeros((q.size, q.max()+1))\n",
    "#             r[np.arange(q.size),q] = 1\n",
    "            \n",
    "            output.append(\n",
    "            Graph(a = graph,y = label)\n",
    "        )\n",
    "            \n",
    "\n",
    "        return output\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "         def make_graph():\n",
    "            \n",
    "            #G = nx.from_scipy_sparse_matrix(self.graph)\n",
    "            #nodes\n",
    "            #x = G.nodes\n",
    "        \n",
    "            #adj matrix\n",
    "            a = graph\n",
    "        \n",
    "            #labels\n",
    "            #y = label\n",
    "        \n",
    "            return Graph(a=a)\n",
    "        return [make_graph() for graph in graphs]\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9ffa113",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(graphs,labels)\n",
    "#dataset.apply(NormalizeAdj())\n",
    "dataset.apply(Degree(9))\n",
    "dataset.apply(OneHotLabels(6,[1,2,3,4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50503f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1.]\n",
      "<class '__main__.MyDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0].y)\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be11b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2  # Learning rate\n",
    "epochs = 100  # Number of training epochs\n",
    "es_patience = 100  # Patience for early stopping\n",
    "batch_size = 32  # Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "733d2580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n",
      "540\n",
      "[ 89 578 221 184 126 482  24 434   3  61]\n"
     ]
    }
   ],
   "source": [
    "# Train/valid/test split\n",
    "idxs = np.random.permutation(len(dataset))\n",
    "split_va, split_te = int(0.8 * len(dataset)), int(0.9 * len(dataset))\n",
    "print(split_va)\n",
    "print(split_te)\n",
    "# idx_tr, idx_va, idx_te = np.split(idxs, [split_va, split_te])\n",
    "idx_tr, idx_va, idx_te = np.split(idxs, [10,15])\n",
    "print(idx_tr)\n",
    "data_tr = dataset[idx_tr]\n",
    "data_va = dataset[idx_va]\n",
    "data_te = dataset[idx_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43ca0588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len Train mask:  480\n",
      "Len Val mask:  60\n",
      "Len Test mask:  60\n"
     ]
    }
   ],
   "source": [
    "print(\"Len Train mask: \" , len(data_tr))\n",
    "print(\"Len Val mask: \" , len(data_va))\n",
    "print(\"Len Test mask: \" , len(data_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f2214f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_tr = DisjointLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = DisjointLoader(data_va, batch_size=batch_size)\n",
    "loader_te = DisjointLoader(data_te, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83fb1fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Build model\n",
    "################################################################################\n",
    "class Net(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphSageConv(32,aggregate = 'max', activation=\"relu\")\n",
    "        self.conv2 = GraphSageConv(32,aggregate = 'max', activation=\"relu\")\n",
    "        self.conv3 = GraphSageConv(32, aggregate = 'max', activation=\"relu\")\n",
    "        self.global_pool = GlobalAvgPool()\n",
    "        self.dense = Dense(dataset.n_labels, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a, i = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.conv2([x, a])\n",
    "        x = self.conv3([x, a])\n",
    "        output = self.global_pool([x, i])\n",
    "        output = self.dense(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "model = Net()\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = CategoricalCrossentropy()\n",
    "model.compile(optimizer = optimizer,\n",
    "         loss = loss_fn,\n",
    "         metrics = ['acc'],\n",
    "        run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24156b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.8035 - acc: 0.1396 - val_loss: 1.7918 - val_acc: 0.2000\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.7900 - acc: 0.1708 - val_loss: 1.7901 - val_acc: 0.1833\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.7862 - acc: 0.2021 - val_loss: 1.7797 - val_acc: 0.1667\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.7681 - acc: 0.2500 - val_loss: 1.7697 - val_acc: 0.1833\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.7494 - acc: 0.2333 - val_loss: 1.7605 - val_acc: 0.2000\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.7279 - acc: 0.2458 - val_loss: 1.7687 - val_acc: 0.1833\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.7216 - acc: 0.2333 - val_loss: 1.7861 - val_acc: 0.1667\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.7098 - acc: 0.2562 - val_loss: 1.7387 - val_acc: 0.2333\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.7103 - acc: 0.2521 - val_loss: 1.7657 - val_acc: 0.1167\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.7029 - acc: 0.2667 - val_loss: 1.7556 - val_acc: 0.1833\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.6887 - acc: 0.2896 - val_loss: 1.7434 - val_acc: 0.2167\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 1.6766 - acc: 0.2896 - val_loss: 1.7510 - val_acc: 0.2167\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.6713 - acc: 0.2812 - val_loss: 1.7499 - val_acc: 0.2167\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.6662 - acc: 0.2979 - val_loss: 1.7400 - val_acc: 0.2500\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.6685 - acc: 0.2917 - val_loss: 1.7441 - val_acc: 0.1667\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 1.6418 - acc: 0.3104 - val_loss: 1.7324 - val_acc: 0.1667\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 1.6356 - acc: 0.3271 - val_loss: 1.7426 - val_acc: 0.2167\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.6187 - acc: 0.3167 - val_loss: 1.7279 - val_acc: 0.2833\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.6218 - acc: 0.3354 - val_loss: 1.7575 - val_acc: 0.3000\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.6005 - acc: 0.3458 - val_loss: 1.7770 - val_acc: 0.2000\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 1.5885 - acc: 0.3604 - val_loss: 1.7651 - val_acc: 0.2333\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 1.5692 - acc: 0.3812 - val_loss: 1.7769 - val_acc: 0.2333\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 1.5699 - acc: 0.3688 - val_loss: 1.7702 - val_acc: 0.2167\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 1.5645 - acc: 0.3875 - val_loss: 1.7705 - val_acc: 0.2667\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.5547 - acc: 0.3688 - val_loss: 1.8172 - val_acc: 0.2167\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.5264 - acc: 0.3917 - val_loss: 1.8347 - val_acc: 0.1667\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.5294 - acc: 0.3917 - val_loss: 1.8588 - val_acc: 0.2000\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 1.5077 - acc: 0.4021 - val_loss: 1.8082 - val_acc: 0.2333\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 1.5152 - acc: 0.4229 - val_loss: 1.8318 - val_acc: 0.2167\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.4928 - acc: 0.4021 - val_loss: 1.7740 - val_acc: 0.2667\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.4744 - acc: 0.4292 - val_loss: 1.8313 - val_acc: 0.2333\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.4543 - acc: 0.4458 - val_loss: 1.8412 - val_acc: 0.2500\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 1.4537 - acc: 0.4271 - val_loss: 1.8510 - val_acc: 0.2167\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.4554 - acc: 0.4208 - val_loss: 1.8397 - val_acc: 0.2333\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.4594 - acc: 0.4458 - val_loss: 1.8562 - val_acc: 0.2000\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.4208 - acc: 0.4542 - val_loss: 1.8829 - val_acc: 0.2167\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.4109 - acc: 0.4563 - val_loss: 1.9626 - val_acc: 0.1500\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.3943 - acc: 0.4667 - val_loss: 1.9461 - val_acc: 0.2667\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.3966 - acc: 0.4875 - val_loss: 1.9697 - val_acc: 0.1833\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.3789 - acc: 0.4979 - val_loss: 1.9575 - val_acc: 0.2333\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.3988 - acc: 0.4729 - val_loss: 1.8732 - val_acc: 0.2333\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.3531 - acc: 0.5000 - val_loss: 1.9635 - val_acc: 0.2667\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.3323 - acc: 0.4979 - val_loss: 2.0118 - val_acc: 0.1500\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 1.3190 - acc: 0.5000 - val_loss: 1.9710 - val_acc: 0.2667\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.3160 - acc: 0.5208 - val_loss: 1.8837 - val_acc: 0.2833\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 1.3005 - acc: 0.5167 - val_loss: 1.9360 - val_acc: 0.3000\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.2896 - acc: 0.5437 - val_loss: 2.0072 - val_acc: 0.2333\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.2873 - acc: 0.5312 - val_loss: 1.9577 - val_acc: 0.3000\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 1.2836 - acc: 0.5375 - val_loss: 1.9207 - val_acc: 0.2667\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.2810 - acc: 0.5229 - val_loss: 2.0362 - val_acc: 0.2667\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 1.2439 - acc: 0.5437 - val_loss: 1.9378 - val_acc: 0.2167\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 1.2524 - acc: 0.5437 - val_loss: 1.9547 - val_acc: 0.2833\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 1.2321 - acc: 0.5792 - val_loss: 2.0057 - val_acc: 0.3167\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.2040 - acc: 0.5833 - val_loss: 2.0224 - val_acc: 0.3167\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 1.1923 - acc: 0.5854 - val_loss: 1.9792 - val_acc: 0.3000\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.1824 - acc: 0.5917 - val_loss: 2.0510 - val_acc: 0.2500\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.1698 - acc: 0.5958 - val_loss: 1.9992 - val_acc: 0.2667\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.1359 - acc: 0.6062 - val_loss: 2.0918 - val_acc: 0.2500\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 1.1236 - acc: 0.6187 - val_loss: 2.0096 - val_acc: 0.3500\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 1.1288 - acc: 0.6021 - val_loss: 2.1157 - val_acc: 0.2667\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 1.1217 - acc: 0.6083 - val_loss: 2.0116 - val_acc: 0.3000\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 60ms/step - loss: 1.0912 - acc: 0.6250 - val_loss: 2.1294 - val_acc: 0.2500\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 1.0936 - acc: 0.6187 - val_loss: 2.1326 - val_acc: 0.2833\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.0847 - acc: 0.6104 - val_loss: 2.0288 - val_acc: 0.3167\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 1.0814 - acc: 0.6104 - val_loss: 2.1807 - val_acc: 0.2333\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 1.0625 - acc: 0.6292 - val_loss: 1.9997 - val_acc: 0.2833\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 1.0447 - acc: 0.6333 - val_loss: 2.3119 - val_acc: 0.2500\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 1.0608 - acc: 0.6313 - val_loss: 2.2015 - val_acc: 0.1667\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.0120 - acc: 0.6458 - val_loss: 2.1255 - val_acc: 0.3000\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.0084 - acc: 0.6750 - val_loss: 2.1492 - val_acc: 0.3667\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.9965 - acc: 0.6500 - val_loss: 2.0566 - val_acc: 0.3000\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.9903 - acc: 0.6521 - val_loss: 2.0266 - val_acc: 0.2833\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.9633 - acc: 0.6854 - val_loss: 2.1711 - val_acc: 0.2833\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.9353 - acc: 0.7229 - val_loss: 2.1049 - val_acc: 0.3667\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.9617 - acc: 0.6812 - val_loss: 2.1736 - val_acc: 0.3500\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.9136 - acc: 0.7083 - val_loss: 2.1521 - val_acc: 0.2333\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.9850 - acc: 0.6458 - val_loss: 2.0409 - val_acc: 0.3000\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.9329 - acc: 0.6917 - val_loss: 2.1720 - val_acc: 0.3000\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.9041 - acc: 0.7000 - val_loss: 2.2804 - val_acc: 0.2333\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.9136 - acc: 0.6833 - val_loss: 2.0688 - val_acc: 0.3167\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.9079 - acc: 0.6854 - val_loss: 2.1149 - val_acc: 0.3167\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.9552 - acc: 0.6521 - val_loss: 2.3700 - val_acc: 0.2667\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.8966 - acc: 0.7000 - val_loss: 2.1721 - val_acc: 0.2500\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.8431 - acc: 0.7417 - val_loss: 2.1985 - val_acc: 0.2667\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 0.8351 - acc: 0.7292 - val_loss: 2.0897 - val_acc: 0.3667\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 0.8417 - acc: 0.7146 - val_loss: 2.2908 - val_acc: 0.2833\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.8726 - acc: 0.7104 - val_loss: 2.2006 - val_acc: 0.2667\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.8112 - acc: 0.7500 - val_loss: 2.3139 - val_acc: 0.2167\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.8296 - acc: 0.7312 - val_loss: 2.2286 - val_acc: 0.2833\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.8024 - acc: 0.7396 - val_loss: 2.1654 - val_acc: 0.2667\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.7987 - acc: 0.7521 - val_loss: 2.2842 - val_acc: 0.3000\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 0.8032 - acc: 0.7479 - val_loss: 2.2495 - val_acc: 0.3000\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.7613 - acc: 0.7604 - val_loss: 2.2623 - val_acc: 0.2667\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.7481 - acc: 0.7917 - val_loss: 2.3883 - val_acc: 0.2500\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.7467 - acc: 0.7833 - val_loss: 2.3377 - val_acc: 0.2333\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.7318 - acc: 0.7792 - val_loss: 2.3460 - val_acc: 0.2667\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.7355 - acc: 0.7750 - val_loss: 2.4780 - val_acc: 0.2333\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.7395 - acc: 0.7750 - val_loss: 2.3389 - val_acc: 0.2667\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.7258 - acc: 0.7792 - val_loss: 2.4293 - val_acc: 0.3000\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.7733 - acc: 0.7500 - val_loss: 2.3467 - val_acc: 0.2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe90c80afa0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    loader_tr.load(),\n",
    "    steps_per_epoch=loader_tr.steps_per_epoch,\n",
    "    validation_data=loader_va.load(),\n",
    "    validation_steps=loader_va.steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    callbacks=[EarlyStopping(patience=es_patience, restore_best_weights=True)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a18981f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"net_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " graph_sage_conv_3 (GraphSag  multiple                 672       \n",
      " eConv)                                                          \n",
      "                                                                 \n",
      " graph_sage_conv_4 (GraphSag  multiple                 2080      \n",
      " eConv)                                                          \n",
      "                                                                 \n",
      " graph_sage_conv_5 (GraphSag  multiple                 2080      \n",
      " eConv)                                                          \n",
      "                                                                 \n",
      " global_avg_pool_1 (GlobalAv  multiple                 0         \n",
      " gPool)                                                          \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  198       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,030\n",
      "Trainable params: 5,030\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f462f551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spektral.data.loaders.DisjointLoader object at 0x7fec51e9f8b0>\n"
     ]
    }
   ],
   "source": [
    "print(loader_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b5b2bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\\ndef train_step(inputs, target):\\n    with tf.GradientTape() as tape:\\n        predictions = model(inputs, training=True)\\n        loss = loss_fn(target, predictions) + sum(model.losses)\\n    gradients = tape.gradient(loss, model.trainable_variables)\\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\\n    acc = tf.reduce_mean(categorical_accuracy(target, predictions))\\n    return loss, acc\\n\\n\\ndef evaluate(loader):\\n    output = []\\n    step = 0\\n    while step < loader.steps_per_epoch:\\n        step += 1\\n        inputs, target = loader.__next__()\\n        pred = model(inputs, training=False)\\n        outs = (\\n            loss_fn(target, pred),\\n            tf.reduce_mean(categorical_accuracy(target, pred)),\\n            len(target),  # Keep track of batch size\\n        )\\n        output.append(outs)\\n        if step == loader.steps_per_epoch:\\n            output = np.array(output)\\n            return np.average(output[:, :-1], 0, weights=output[:, -1])\\n\\n\\nepoch = step = 0\\nbest_val_loss = np.inf\\nbest_weights = None\\npatience = es_patience\\nresults = []\\nfor batch in loader_tr:\\n    step += 1\\n    loss, acc = train_step(*batch)\\n    results.append((loss, acc))\\n    if step == loader_tr.steps_per_epoch:\\n        step = 0\\n        epoch += 1\\n\\n        # Compute validation loss and accuracy\\n        val_loss, val_acc = evaluate(loader_va)\\n        print(\\n            \"Ep. {} - Loss: {:.3f} - Acc: {:.3f} - Val loss: {:.3f} - Val acc: {:.3f}\".format(\\n                epoch, *np.mean(results, 0), val_loss, val_acc\\n            )\\n        )\\n\\n        # Check if loss improved for early stopping\\n        if val_loss < best_val_loss:\\n            best_val_loss = val_loss\\n            patience = es_patience\\n            print(\"New best val_loss {:.3f}\".format(val_loss))\\n            best_weights = model.get_weights()\\n        else:\\n            patience -= 1\\n            if patience == 0:\\n                print(\"Early stopping (best val_loss: {})\".format(best_val_loss))\\n                break\\n        results = []\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
    "def train_step(inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    acc = tf.reduce_mean(categorical_accuracy(target, predictions))\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def evaluate(loader):\n",
    "    output = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        outs = (\n",
    "            loss_fn(target, pred),\n",
    "            tf.reduce_mean(categorical_accuracy(target, pred)),\n",
    "            len(target),  # Keep track of batch size\n",
    "        )\n",
    "        output.append(outs)\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1])\n",
    "\n",
    "\n",
    "epoch = step = 0\n",
    "best_val_loss = np.inf\n",
    "best_weights = None\n",
    "patience = es_patience\n",
    "results = []\n",
    "for batch in loader_tr:\n",
    "    step += 1\n",
    "    loss, acc = train_step(*batch)\n",
    "    results.append((loss, acc))\n",
    "    if step == loader_tr.steps_per_epoch:\n",
    "        step = 0\n",
    "        epoch += 1\n",
    "\n",
    "        # Compute validation loss and accuracy\n",
    "        val_loss, val_acc = evaluate(loader_va)\n",
    "        print(\n",
    "            \"Ep. {} - Loss: {:.3f} - Acc: {:.3f} - Val loss: {:.3f} - Val acc: {:.3f}\".format(\n",
    "                epoch, *np.mean(results, 0), val_loss, val_acc\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Check if loss improved for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = es_patience\n",
    "            print(\"New best val_loss {:.3f}\".format(val_loss))\n",
    "            best_weights = model.get_weights()\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                print(\"Early stopping (best val_loss: {})\".format(best_val_loss))\n",
    "                break\n",
    "        results = []\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234c1051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
